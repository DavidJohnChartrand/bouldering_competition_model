{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e9bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7420a52",
   "metadata": {},
   "outputs": [],
   "source": [
    " browser = Browser('chrome', incognito=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caeb0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info(link):\n",
    "\n",
    "    browser.visit(link)\n",
    "    per_info_soupy = soup(browser.html, \"html.parser\")\n",
    "    athlete = per_info_soupy.find_all(\"div\", class_=\"athlete\")\n",
    "    country = athlete[0].find(\"div\", class_=\"country\").find(\"span\").text\n",
    "    name = athlete[0].find(\"h1\", class_=\"name\").text\n",
    "    age = athlete[0].find(\"span\", class_=\"age\").text\n",
    "    hometown = athlete[0].find(\"span\", class_=\"hometown\").text\n",
    "    info = per_info_soupy.find(\"div\", class_=\"personal-info\")\n",
    "    athlete_info = info.find(\"div\",class_=\"text\")\n",
    "    catergories = []\n",
    "    data = []\n",
    "    Athlete_infos = []\n",
    "    try:   \n",
    "        for i in athlete_info:\n",
    "            if i[\"class\"][0]==\"subtitle\":\n",
    "                catergories.append(i.text)\n",
    "            else:\n",
    "                data.append(i.text)\n",
    "\n",
    "        athlete_infos = {catergories[i]: data[i] for i in range(len(catergories))}\n",
    "        Athlete_infos.append(athlete_infos)\n",
    "        \n",
    "    except:\n",
    "        \"NO PERSONAL INFORMATION\"\n",
    "    INFO =[country,name,age, Athlete_infos]\n",
    "    return INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96f6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climbers_stats(link):\n",
    "    browser.visit(link)\n",
    "    soupy = soup(browser.html, \"html.parser\")\n",
    "    iframe = soupy.find('iframe')\n",
    "    iframe_url = iframe['data-src']\n",
    "    iframe_url\n",
    "    browser.visit(iframe_url)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae2e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_table():\n",
    "    iframesoup = soup(browser.html, \"html.parser\")\n",
    "    table = iframesoup.find('table')\n",
    "    climbers =[]\n",
    "    for row in table.find_all('tr'):\n",
    "        try:\n",
    "            link = row.find_all('td')[1].find(\"a\")[\"href\"]\n",
    "            climber = personal_info(link)\n",
    "            climbers.append(climber)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    return climbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0807ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climbers(url):\n",
    "    climbers_stats(url)\n",
    "    sleep(1)\n",
    "    climbers = loop_table()\n",
    "    return climbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc78e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_climber_df(url):\n",
    "    competitors = climbers(url)\n",
    "    df = pd.DataFrame(competitors)\n",
    "    df = clean_climber_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fbb07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_climber_df(df):\n",
    "    personal_info_list = pd.json_normalize(df[3])\n",
    "    personl_info_df = pd.json_normalize(personal_info_list[0])\n",
    "    df[1]=df[1].str.strip()\n",
    "    name_df = df[1].str.split(\" \",expand=True)\n",
    "    for i in range(len(name_df.columns)):\n",
    "        name_df.rename(columns={i:f\"name_{i}\"}, inplace=True)\n",
    "    df[2]=df[2].str.strip(\"Age: \")\n",
    "    df = df.merge(name_df, left_index=True, right_index=True)\n",
    "    df = df.drop([1,3],axis=1)\n",
    "    df = df.merge(personl_info_df,left_index=True, right_index=True)\n",
    "    df = df.rename(columns={0:\"country\",2:\"age\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6ce521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def women():\n",
    "    women_url = 'https://www.ifsc-climbing.org/index.php/world-competition/calendar?task=ranking-complete&category=7'\n",
    "    df = create_climber_df(women_url)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380bc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def men():\n",
    "    men_url = 'https://www.ifsc-climbing.org/index.php/world-competition/calendar?task=ranking-complete&category=3'\n",
    "    df = create_climber_df(men_url)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f3a3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    women_df = women()\n",
    "    men_df = men()\n",
    "    browser.quit()\n",
    "    \n",
    "    women_df.to_csv('women_competitors.csv', index=False)\n",
    "    men_df.to_csv('men_competitors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34dfd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "417f20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "class IFSCBloudererScraper():\n",
    "    \"\"\"\n",
    "    Define a class for the scrapper from the IFSC website\n",
    "    (ifsc-climbing.org)\n",
    "    scrapes both male and female competitors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, debug=False):\n",
    "        \"\"\"\n",
    "        Initialize a scraper object with the Chrome Browser instance\n",
    "        Input:\n",
    "            debug - Indicates whther this is a debuf instance\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Create Chrome isntance\n",
    "        browser = Browser('chrome', incognito=True)\n",
    "        \n",
    "        sleep(1)\n",
    "    \n",
    "    def personal_info(self, link):\n",
    "        \"\"\"\n",
    "        Returns the information from an individual Athletes page on the \n",
    "        IFSC's website\n",
    "        input:\n",
    "            link - \n",
    "        \"\"\"\n",
    "        \n",
    "        browser.visit(link)\n",
    "        per_info_soupy = soup(browser.html, \"html.parser\")\n",
    "        athlete = per_info_soupy.find_all(\"div\", class_=\"athlete\")\n",
    "        country = athlete[0].find(\"div\", class_=\"country\").find(\"span\").text\n",
    "        name = athlete[0].find(\"h1\", class_=\"name\").text\n",
    "        age = athlete[0].find(\"span\", class_=\"age\").text\n",
    "        hometown = athlete[0].find(\"span\", class_=\"hometown\").text\n",
    "        info = per_info_soupy.find(\"div\", class_=\"personal-info\")\n",
    "        athlete_info = info.find(\"div\",class_=\"text\")\n",
    "        catergories = []\n",
    "        data = []\n",
    "        Athlete_infos = []\n",
    "        try:   \n",
    "            for i in athlete_info:\n",
    "                if i[\"class\"][0]==\"subtitle\":\n",
    "                    catergories.append(i.text)\n",
    "                else:\n",
    "                    data.append(i.text)\n",
    "\n",
    "            athlete_infos = {catergories[i]: data[i] for i in range(len(catergories))}\n",
    "            Athlete_infos.append(athlete_infos)\n",
    "\n",
    "        except:\n",
    "            \"NO PERSONAL INFORMATION\"\n",
    "        INFO =[country,name,age, Athlete_infos]\n",
    "        return INFO\n",
    "    \n",
    "    def climbers_stats(self, link):\n",
    "        browser.visit(link)\n",
    "        soupy = soup(browser.html, \"html.parser\")\n",
    "        iframe = soupy.find('iframe')\n",
    "        iframe_url = iframe['data-src']\n",
    "        iframe_url\n",
    "        browser.visit(iframe_url)\n",
    "        return\n",
    "    \n",
    "    def loop_table(self):\n",
    "        iframesoup = soup(browser.html, \"html.parser\")\n",
    "        table = iframesoup.find('table')\n",
    "        climbers =[]\n",
    "        for row in table.find_all('tr'):\n",
    "            try:\n",
    "                link = row.find_all('td')[1].find(\"a\")[\"href\"]\n",
    "                climber = self.personal_info(link)\n",
    "                climbers.append(climber)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "        return climbers\n",
    "\n",
    "    def climbers(self, url):\n",
    "        self.climbers_stats(url)\n",
    "        sleep(1)\n",
    "        climbers = self.loop_table()\n",
    "        return climbers\n",
    "    \n",
    "    def create_climber_df(self, url):\n",
    "        competitors = self.climbers(url)\n",
    "        df = pd.DataFrame(competitors)\n",
    "        df = self.clean_climber_df(df)\n",
    "        return df\n",
    "    \n",
    "    def clean_climber_df(self, df):\n",
    "        personal_info_list = pd.json_normalize(df[3])\n",
    "        personl_info_df = pd.json_normalize(personal_info_list[0])\n",
    "        df[1]=df[1].str.strip()\n",
    "        name_df = df[1].str.split(\" \",expand=True)\n",
    "        for i in range(len(name_df.columns)):\n",
    "            name_df.rename(columns={i:f\"name_{i}\"}, inplace=True)\n",
    "        df[2]=df[2].str.strip(\"Age: \")\n",
    "        df = df.merge(name_df, left_index=True, right_index=True)\n",
    "        df = df.drop([1,3],axis=1)\n",
    "        df = df.merge(personl_info_df,left_index=True, right_index=True)\n",
    "        df = df.rename(columns={0:\"country\",2:\"age\"})\n",
    "        return df\n",
    "    \n",
    "    def women(self):\n",
    "        women_url = 'https://www.ifsc-climbing.org/index.php/world-competition/calendar?task=ranking-complete&category=7'\n",
    "        df = self.create_climber_df(women_url)\n",
    "        return df\n",
    "\n",
    "    def men(self):\n",
    "        men_url = 'https://www.ifsc-climbing.org/index.php/world-competition/calendar?task=ranking-complete&category=3'\n",
    "        df = self.create_climber_df(men_url)\n",
    "        return df\n",
    "    \n",
    "    def scrape(self):\n",
    "        women_df = self.women()\n",
    "        men_df = self.men()\n",
    "        browser.quit()\n",
    "\n",
    "        women_df.to_csv('women_competitors.csv', index=False)\n",
    "        men_df.to_csv('men_competitors.csv', index=False)\n",
    "        \n",
    "def main():\n",
    "    # Create scraper object\n",
    "    scraper = IFSCBloudererScraper()\n",
    "    \n",
    "    #Run scraper\n",
    "    scraper.scrape()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ce9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
